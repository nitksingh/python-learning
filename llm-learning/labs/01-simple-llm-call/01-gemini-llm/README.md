# 01: Gemini LLM - Google's Gemini API

Learn to call Google's Gemini API (free tier available).

---

## ğŸ“ What You'll Learn

### **File 1: `01_simple_gemini_call.py`** (Basic API Call)

**Learning Flow:**
1. Configure SDK with API key
2. List available models
3. Select a model
4. Call the model with a prompt (message)
5. Print the response

**ğŸ’¡ Key Concepts for Lab 1:**

#### **What is a Prompt?**

A **prompt** is the text/instruction you send to an LLM to get a response.

**Example:**
```python
prompt = "What's the capital of France?"
response = model.generate_content(prompt)
```

#### **Message Roles (3 Types)**

Production AI apps use **3 message roles** for better control:

| Role | Purpose | Example |
|------|---------|---------|
| **system** | Developer's instructions, rules, behavior | "You are a helpful math tutor." |
| **user** | End user's question/input | "What is 25 Ã— 4?" |
| **assistant** | AI's previous responses (for history) | "25 Ã— 4 = 100. Here's how..." |

**Why roles matter:**
- âœ… **Security** - Prevent prompt injection attacks
- âœ… **History** - Multi-turn conversation management
- âœ… **Structure** - Clean separation of instructions vs. data

**ğŸ“š Want to learn more about prompting?**

ğŸ‘‰ **See [Lab 02: Prompt Engineering â†’ 01-basic-prompting](../../02-prompt-engineering/01-basic-prompting/README.md)** for:
- Detailed role explanations with examples
- Security patterns (prompt injection defense)
- Core prompting patterns (zero-shot, few-shot, chain-of-thought)
- Parameters guide (temperature, top_p, etc.)
- Production best practices

---

### **File 2: `02_advanced_gemini_call.py`** (Production-Ready Code)

**Learning Flow:**
1. Structured code with classes
2. Input validation (check empty/long inputs)
3. Error handling (API errors, network issues)
4. Parameters:
   - `temperature` - Controls randomness (0.0 = consistent, 1.0 = creative)
   - `max_tokens` - Limits response length (prevents long responses)
5. `save_history` - **THE PROBLEM IT SOLVES:**

   **âš ï¸ CRITICAL CONCEPT: LLMs are STATELESS!**
   
   LLMs (like Gemini, GPT-4, Claude) have **NO MEMORY** between API calls.
   Each call is completely independent - the model doesn't "remember" anything.
   
   **Visual Flow (How History is Actually Sent):**
   ```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  WITHOUT save_history (Each call is independent)        â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  Call 1: Send to model: "Capital of France?"           â”‚
   â”‚          Model replies: "Paris"                         â”‚
   â”‚                                                         â”‚
   â”‚  Call 2: Send to model: "What's its population?"       â”‚
   â”‚          Model replies: "Population of what?" âŒ        â”‚
   â”‚          (Model never saw previous question!)           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  WITH save_history (Context sent back each time)        â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  Call 1: Send to model: "Capital of France?"           â”‚
   â”‚          Model replies: "Paris"                         â”‚
   â”‚          â†“ Save to conversation_history[]              â”‚
   â”‚          conversation_history = [                       â”‚
   â”‚            {user: "Capital of France?", bot: "Paris"}   â”‚
   â”‚          ]                                              â”‚
   â”‚                                                         â”‚
   â”‚  Call 2: Build prompt from history + new question:     â”‚
   â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
   â”‚          â”‚ User: Capital of France?             â”‚      â”‚
   â”‚          â”‚ Assistant: Paris                     â”‚      â”‚
   â”‚          â”‚ User: What's its population?         â”‚      â”‚
   â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
   â”‚          Send this FULL context to model               â”‚
   â”‚          Model replies: "Paris has 2.1M people" âœ…     â”‚
   â”‚          (Model sees the word "Paris" in history!)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```
   
   **The Key Mechanism (in code):**
   ```python
   # Step 1: User asks first question
   response1 = generate_response("Capital of France?", save_history=True)
   # Internally saves: {user: "Capital...", bot: "Paris"}
   
   # Step 2: User asks follow-up
   # Code builds FULL prompt including history:
   full_prompt = """
   User: Capital of France?
   Assistant: Paris
   User: What's its population?
   """
   # THIS full_prompt is sent to model, so it sees "Paris"!
   response2 = generate_response("What's its population?", save_history=True)
   ```
   
   **When to use:** Chatbots, Q&A apps, anything needing context

6. `save_history_to_file` - **THE PROBLEM IT SOLVES:**

   **âŒ WITHOUT saving to file:**
   ```
   User reports: "Bot gave wrong answer!"
   You: "What did you ask?" 
   User: "I don't remember exactly..." 
   You: âŒ Can't debug or reproduce the issue
   ```
   
   **âœ… WITH saving to file:**
   ```
   User reports: "Bot gave wrong answer!"
   You: Open conversation_123.txt
   You: âœ… See exact conversation, find the bug!
   ```
   
   **Real-world uses:**
   - **Debug:** See what went wrong in production
   - **Audit:** Legal/compliance requirements (who said what when)
   - **Training:** Improve prompts by reviewing real conversations
   - **Share:** Send conversation logs to teammates
   
   **Visual Example:**
   ```
   User Chat â†’ save_history_to_file() â†’ conversation_2024-11-23.txt
   
   File contents:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ [2024-11-23 10:30:15] User: Capital of France?â”‚
   â”‚ [2024-11-23 10:30:16] Bot: Paris              â”‚
   â”‚ [2024-11-23 10:31:20] User: Population?       â”‚
   â”‚ [2024-11-23 10:31:21] Bot: 2.1M people        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```
   
   **When to use:** Production apps, customer support, legal/compliance

**ğŸ’¡ Additional Key Concepts for Lab 2:**

#### **Parameters (Brief)**

- **`temperature`** (0.0 - 1.0): Controls randomness
  - `0.0` = Consistent, deterministic responses
  - `0.7` = Balanced (default)
  - `1.0` = Creative, varied responses

- **`max_tokens`**: Limits response length
  - Prevents overly long responses
  - Controls costs (charged per token)

---

## ğŸš€ How to Run

### **Setup (One-time)**

```bash
# 1. Run setup script
./setup.sh

# 2. Add your API key to .env
nano .env
# Add: GEMINI_API_KEY=your_key_here
# Get free key from: https://aistudio.google.com/

# 3. Activate environment
source venv/bin/activate
```

### **Run Programs**

```bash
# Basic call (start here!)
python 01_simple_gemini_call.py

# Advanced call (production-ready)
python 02_advanced_gemini_call.py
```

---

## âœ… What You've Learned

**From Lab 1 (`01_simple_gemini_call.py`):**
- âœ… What a prompt is and how to send it
- âœ… 3 message roles (system, user, assistant) and why they matter
- âœ… How to authenticate and make basic API calls

**From Lab 2 (`02_advanced_gemini_call.py`):**
- âœ… **CRITICAL:** LLMs are STATELESS (no memory between calls!)
- âœ… How to "fake" memory using `save_history`
- âœ… Why persist conversations with `save_history_to_file`
- âœ… Production patterns (validation, error handling, parameters)

---

## ğŸ¯ Next Steps

After mastering these:
1. Try `02-any-llm` to work with multiple LLM providers
2. Explore `02-prompt-engineering` for advanced techniques

---

**Duration:** 15-30 minutes | **Level:** Beginner | **Cost:** FREE
